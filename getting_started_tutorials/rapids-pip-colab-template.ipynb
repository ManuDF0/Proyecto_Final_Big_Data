{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/rapids-pip-colab-template.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Install RAPIDS into Colab\"/>\n",
        "</a>\n",
        "\n",
        "# RAPIDS cuDF is now already on your Colab instance!\n",
        "RAPIDS cuDF is preinstalled on Google Colab and instantly accelerates Pandas with zero code changes. [You can quickly get started with our tutorial notebook](https://nvda.ws/rapids-cudf). This notebook template is for users who want to utilize the full suite of the RAPIDS libraries for their workflows on Colab.  \n",
        "\n",
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "You can check the output of `!nvidia-smi` to check which GPU you have.  Please uncomment the cell below if you'd like to do that.  Currently, RAPIDS runs on all available Colab GPU instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67T0090Jk2KL"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_v33LnDVNo3"
      },
      "source": [
        "#Setup:\n",
        "This set up script:\n",
        "\n",
        "1. Checks to make sure that the GPU is RAPIDS compatible\n",
        "1. Pip Installs the RAPIDS' libraries, which are:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuxFilter\n",
        "  1. cuCIM\n",
        "  1. xgboost\n",
        "\n",
        "# Controlling Which RAPIDS Version is Installed\n",
        "This line in the cell below, `!python rapidsai-csp-utils/colab/pip-install.py`, kicks off the RAPIDS installation script.  You can control the RAPIDS version installed by adding either `latest`, `nightlies` or the default/blank option.  Example:\n",
        "\n",
        "`!python rapidsai-csp-utils/colab/pip-install.py <option>`\n",
        "\n",
        "You can now tell the script to install:\n",
        "1. **RAPIDS + Colab Default Version**, by leaving the install script option blank (or giving an invalid option), adds the rest of the RAPIDS libraries to the RAPIDS cuDF library preinstalled on Colab.  **This is the default and recommended version.**  Example: `!python rapidsai-csp-utils/colab/pip-install.py`\n",
        "1. **Latest known working RAPIDS stable version**, by using the option `latest` upgrades all RAPIDS labraries to the latest working RAPIDS stable version.  Usually early access for future RAPIDS+Colab functionality - some functionality may not work, but can be same as the default version. Example: `!python rapidsai-csp-utils/colab/pip-install.py latest`\n",
        "1. **the current nightlies version**, by using the option, `nightlies`, installs current RAPIDS nightlies version.  For RAPIDS Developer use - **not recommended/untested**.  Example: `!python rapidsai-csp-utils/colab/pip-install.py nightlies`\n",
        "\n",
        "\n",
        "**This will complete in about 5-6 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72580b57-a2b1-4d29-8aaf-ac51c4b85fd9"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n",
            "Installing RAPIDS remaining 24.10.* libraries\n",
            "Using Python 3.11.11 environment at /usr\n",
            "Audited 11 packages in 91ms\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJMJ6BulmMn"
      },
      "source": [
        "# RAPIDS is now installed on Colab.  \n",
        "You can copy your code into the cells below or use the below to validate your RAPIDS installation and version.  \n",
        "# Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLrk46BllED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a75f7bc8-ef43-4d97-eed6-e6b49e334b61"
      },
      "source": [
        "import cudf\n",
        "cudf.__version__"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.10.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "cuml.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgAFgI15ddf6",
        "outputId": "e9c69431-33e1-40f5-92fe-7ae9e9976374"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.10.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cugraph\n",
        "cugraph.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JOCMWaUal1fI",
        "outputId": "24b14a29-7f3d-422a-b675-72902f919414"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.10.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuspatial\n",
        "cuspatial.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AnmtYjzvVTtv",
        "outputId": "33c4f916-e403-478c-f786-b0f5058c5199"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.10.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuxfilter\n",
        "cuxfilter.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CYjcARDFVWWD",
        "outputId": "66f7a1b0-9293-4bbe-fa88-d6f50853e803"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.10.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-contrib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV, LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, mean_squared_error\n",
        "import numpy as np\n",
        "import joblib"
      ],
      "metadata": {
        "id": "9GE3Jvj8d3_M"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1E3mW0xMmC7JBnD20H4qZA0XPlSruVpJ2\n",
        "!gdown 1sce_ashbiRGErbm7OXr8w2BGGy9u5jon\n",
        "!gdown 11mD6Go_WSF6ksAH8lwW8alEeUM8WiUHx\n",
        "!gdown 1hzopdXc-GiZ03Uqf556A3IgGh87zn3e6"
      ],
      "metadata": {
        "id": "KsD9PYkNINn6",
        "outputId": "4f111ae5-6ece-48eb-f4d1-24622301cd92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1E3mW0xMmC7JBnD20H4qZA0XPlSruVpJ2\n",
            "To: /content/constancia_inscripcion.parquet\n",
            "100% 10.6M/10.6M [00:00<00:00, 65.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sce_ashbiRGErbm7OXr8w2BGGy9u5jon\n",
            "To: /content/creditos_hist.parquet\n",
            "100% 62.7M/62.7M [00:00<00:00, 105MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11mD6Go_WSF6ksAH8lwW8alEeUM8WiUHx\n",
            "To: /content/principales_variables.parquet\n",
            "100% 124k/124k [00:00<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hzopdXc-GiZ03Uqf556A3IgGh87zn3e6\n",
            "To: /content/sh_emae_mensual_base2004.xls\n",
            "100% 71.7k/71.7k [00:00<00:00, 76.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_parquet('./creditos_hist.parquet')"
      ],
      "metadata": {
        "id": "uwtcMzFjImCm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminamos las situaciones 0, que indican que el crédito ya fue pagado\n",
        "data = data.loc[data['situacion'] != 0]\n",
        "data = data.drop('denominacion', axis = 1) # Elimino la columna con las razones sociales para ahorrar RAM"
      ],
      "metadata": {
        "id": "8m_G0xDyIrs6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Una variable que puede ser de interés es cuantos créditos tiene una empresa en un momento dado del tiempo\n",
        "counts = data.groupby(['identificacion', 'periodo']).size().reset_index(name='n_creditos')\n",
        "\n",
        "# También nos interesa cuanta plata debe una empresa en cada momento dado\n",
        "sums = data.groupby(['identificacion', 'periodo'], as_index=True)['monto'].sum().reset_index(name='sum_montos')\n",
        "\n",
        "# La literatura indica que también importa la duración de la relación empresa-banco, por lo que contamos la cantidad\n",
        "# de periodos que aparece cada par: empresa-banco\n",
        "period_counts = data.groupby(['identificacion', 'entidad']).size().reset_index(name='n_periodos')\n",
        "\n",
        "# Definimos como default cuando el crédito se encuentra en situación 4 o 5, por lo que creamos la dummy de default\n",
        "# Esta es nuestra variable dependiente\n",
        "data['default'] = (data['situacion'] >= 4).astype(int)"
      ],
      "metadata": {
        "id": "iAJY2KOwJABD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queremos predecir el default el periodo siguienre\n",
        "data['default_lag'] = data.groupby(['identificacion', 'entidad'])['default'].shift(1) # Lag a la variable default\n",
        "data = data.dropna(subset=['default_lag']) # Eliminamos las observaciones que no tienen variable dependiente\n",
        "data['default_lag'] = data['default_lag'].astype(int) # Cambio el dtype de la variable de interés"
      ],
      "metadata": {
        "id": "FJfdJGK_JEL9",
        "outputId": "4b9f66d7-f6cf-4605-dbba-abf0cf534cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-67778628611e>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['default_lag'] = data['default_lag'].astype(int) # Cambio el dtype de la variable de interés\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sort_values(by=['identificacion', 'periodo'])\n",
        "data['prev_default'] = (\n",
        "    data.groupby('identificacion')['default']\n",
        "    .transform(lambda x: x.cumsum().clip(upper=1))\n",
        ") # Armamos una variable que indique si en algún momento de su historia, esa empresa tuvo un crédito en default"
      ],
      "metadata": {
        "id": "oY3a1iEbJG53"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"sin_historial\"] = (\n",
        "    data.groupby(\"identificacion\")[\"periodo\"]\n",
        "    .transform(\"rank\", method=\"first\") == 1).astype(int)"
      ],
      "metadata": {
        "id": "8O-2k2eIJJ71"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregamos las nuevas variables al dataframe\n",
        "data = data.merge(counts, on=['identificacion', 'periodo'], how='left')\n",
        "data = data.merge(sums, on=['identificacion', 'periodo'], how='left')\n",
        "data = data.merge(period_counts, on=['identificacion', 'entidad'], how='left')\n",
        "\n",
        "del sums, counts, period_counts # Para ahorrar RAM"
      ],
      "metadata": {
        "id": "68JDv9x-JMFA"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Por último, la literatura también resalta que la intensidad de la relación empresa-banco es relevante\n",
        "# Usamos como proxy para la intensidad la proporción del monto adeudado con un banco sobre el total adeudado\n",
        "data['monto_relativo'] = data['monto'] / data['sum_montos']"
      ],
      "metadata": {
        "id": "-UkKLWW2JQx3"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.loc[data['periodo'] > '202310']"
      ],
      "metadata": {
        "id": "n5Au9cPKJSsg"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elijo aleatoriamente un porcentaje de las empresas de la población\n",
        "np.random.seed(42)\n",
        "cuits = data['identificacion'].unique()\n",
        "moneda = np.random.binomial(1, 0.1, len(cuits)) # Es como tirar una moneda sesgada para que agarre un porcentaje arbitrario de las empresas\n",
        "cuits_aleatorios = cuits[moneda == 1] # Estos son los cuits con los que me voy a quedar\n",
        "data = data.loc[data['identificacion'].isin(cuits_aleatorios)] # Me quedo unicamente con las obs que tienen un cuit dentro de los seleccionados aleatoriamente"
      ],
      "metadata": {
        "id": "VjsK0e9UJUYG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pv = pd.read_parquet('./principales_variables.parquet') # Datos de principales variables monetarias provenientes de la API del BCRA\n",
        "pv.reset_index(inplace= True) # el index es la fecha, así que lo paso a columna\n",
        "pv['fecha'] = pd.to_datetime(pv['fecha']) # paso la nueva columna al formato correcto\n",
        "pv['periodo'] = pv['fecha'].dt.strftime('%Y%m') # armo una variable llamada periodo igual a la que tengo en los datos de la Central de Deudores\n",
        "pv = pv.drop('fecha', axis = 1).groupby('periodo').agg(['mean', 'std']) # elimino la de \"fecha\" porque no me interesan los datos diarios\n",
        "# Me quedo únicamente con los promedios por mes y también calculo el desvío estándar\n",
        "pv.columns = ['_'.join(col).strip() for col in pv.columns] # Renombro las columnas para que sea más prolijo\n",
        "pv.reset_index(inplace= True) # Vuelvo a agregar la columna periodo\n",
        "pv = pv.loc[pv['periodo'].astype(int) <= 202411] # En la Central de Deudores tenemos datos hasta 202410\n",
        "pv = pv.dropna(axis = 1) # Elimino las columnas con NAs"
      ],
      "metadata": {
        "id": "SZAe40mwJbbq"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pv = pv.sort_values(\"periodo\")\n",
        "\n",
        "pv[\"inflacion_acumulada\"] = (1 + pv[\"Inflación mensual (variación en %)_mean\"]/100).cumprod()\n",
        "\n",
        "columnas = pv.columns.tolist()\n",
        "tasas = [3, 4, 5, 6, 28 ,29, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
        "excluir = [0, 11, 12, 25, 26, 27, 34, 35, 58, 59, 60, 61, 62, 63, 64]\n",
        "\n",
        "columnas_excluir = [columnas[i] for i in excluir]\n",
        "columnas_tasas = [columnas[i] for i in tasas]\n",
        "\n",
        "columnas_nominales = [col for col in pv.columns if col not in columnas_excluir and col not in columnas_tasas]\n",
        "columnas_tasas = [col for col in pv.columns if col in tasas and col not in columnas_excluir]\n",
        "\n",
        "for col in columnas_nominales:\n",
        "    nombre_col_real = f\"{col}_real\"\n",
        "    pv[nombre_col_real] = pv[col] / pv['inflacion_acumulada']\n",
        "\n",
        "for col in columnas_tasas:\n",
        "    nombre_col_real = f\"{col}_real\"\n",
        "    pv[nombre_col_real] = pv[col]/(100*pv['inflacion_acumulada'])"
      ],
      "metadata": {
        "id": "S4OOp0qaJgAE"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.merge(pv, on = 'periodo', how = 'left') # Junto las principales variables monetarias con la Central de Deudores"
      ],
      "metadata": {
        "id": "dfVh_Ct8JjPe"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['monto_real'] = data['monto']/data['inflacion_acumulada']\n",
        "data['sum_montos_real'] = data['sum_montos']/data['inflacion_acumulada']\n",
        "data.drop('inflacion_acumulada', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "wJRGSe13JoHD"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emae = pd.read_excel('./sh_emae_mensual_base2004.xls', index_col=[0,1])\n",
        "meses_a_numeros = {\n",
        "    'Enero': '01', 'Febrero': '02', 'Marzo': '03', 'Abril': '04',\n",
        "    'Mayo': '05', 'Junio': '06', 'Julio': '07', 'Agosto': '08',\n",
        "    'Septiembre': '09', 'Octubre': '10', 'Noviembre': '11', 'Diciembre': '12'\n",
        "}\n",
        "emae = emae.reset_index()\n",
        "emae['level_1'] = emae['level_1'].map(meses_a_numeros)\n",
        "emae['periodo'] = emae['Período'].astype(str) + emae['level_1']\n",
        "emae = emae.drop(columns=['level_1', 'Período'])"
      ],
      "metadata": {
        "id": "8Oa4Z3IrJreO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.merge(emae, on = 'periodo', how= 'left')"
      ],
      "metadata": {
        "id": "PVPOvkS4J0qw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arca = pd.read_parquet('./constancia_inscripcion.parquet') # Cargo los datos de la constancia de inscripción de ARCA"
      ],
      "metadata": {
        "id": "5pBDM9wNJ1Ks"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuits_arca = set(arca['identificacion'])\n",
        "cuits_bcra = set(data['identificacion'])\n",
        "\n",
        "faltan = list(cuits_bcra - cuits_arca)\n",
        "\n",
        "data['sin_arca'] = (data['identificacion'].isin(faltan)).astype(int)"
      ],
      "metadata": {
        "id": "3rmVaslFJ2-u"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ponemos bien el tipo de dato para las columnas categóricas, así el get_dummies funciona bien\n",
        "data['identificacion'] = data['identificacion'].astype('category')\n",
        "data['entidad'] = data['entidad'].astype('category')\n",
        "data['situacion'] = data['situacion'].astype('category')\n",
        "data['default'] = data['default'].astype('category')\n",
        "data['periodo'] = data['periodo'].astype('category')\n",
        "data['default_lag'] = data['default_lag'].astype('category')\n",
        "data['prev_default'] = data['prev_default'].astype('category')\n",
        "data['sin_arca'] = data['sin_arca'].astype('category')"
      ],
      "metadata": {
        "id": "nnLxNb1nJ5Mj"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation\n",
        "data = data.sort_values(by='periodo') # Ordeno de acuerdo a la fecha\n",
        "data = data.reset_index().drop(columns= 'index')\n",
        "split_index = int(len(data) * 0.8) # El 80% de las observaciones más antiguas\n",
        "train_indices = data.iloc[:split_index].index # Estos son los índices con los que después voy a separar en test y train\n",
        "test_indices = data.iloc[split_index:].index"
      ],
      "metadata": {
        "id": "vIYOk_iiJ66f"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = data['default_lag']"
      ],
      "metadata": {
        "id": "r9wt1wnuJ_BU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boolean_columns = data.select_dtypes(include='object').columns # Estas son las columnas que ya están en el formato correcto\n",
        "columnas = ['entidad', 'monto', 'n_creditos', 'sum_montos', 'n_periodos', 'monto_relativo', 'sin_arca', 'default', 'prev_default', 'sin_historial', 'periodo', 'monto_real', 'sum_montos_real'] # Algunas de las variables independientes del modelo\n",
        "pv.set_index('periodo', inplace= True)\n",
        "pv.drop('inflacion_acumulada', axis = 1, inplace = True)\n",
        "columnas.extend(pv.columns) # Todas las columnas de las principales variables monetarias\n",
        "emae.set_index('periodo', inplace= True)\n",
        "columnas.extend(emae.columns) # Todas las columnas de emae"
      ],
      "metadata": {
        "id": "El0zAN-xKAW9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_encode = [col for col in columnas if col not in boolean_columns] # Una lista con las columnas que no tengo que meter en \"get_dummies\"\n",
        "X_encoded = pd.get_dummies(data[columns_to_encode], drop_first=True) # Meto las columnas en get_dummies\n",
        "X = pd.concat([X_encoded, data[boolean_columns]], axis=1) # Junto todas las variables independientes en un solo df\n",
        "\n",
        "del columns_to_encode, X_encoded"
      ],
      "metadata": {
        "id": "k57cEQUBKB1h"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = cudf.DataFrame(X)\n",
        "Y = cudf.DataFrame(Y)"
      ],
      "metadata": {
        "id": "QCEvcaFkKxxh"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "Y = scaler.fit_transform(Y)"
      ],
      "metadata": {
        "id": "jLTONtMoL4y8",
        "outputId": "0350380e-3b32-44cd-ad66-649ecf97e309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\nTo explicitly construct a host matrix, consider using .to_numpy().",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-6e32beab4c16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \"\"\"\n\u001b[1;32m    929\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/utils/performance_tracking.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     )\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cudf/core/frame.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_performance_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    395\u001b[0m             \u001b[0;34m\"Implicit conversion to a host NumPy array via __array__ is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;34m\"allowed, To explicitly construct a GPU matrix, consider using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Implicit conversion to a host NumPy array via __array__ is not allowed, To explicitly construct a GPU matrix, consider using .to_cupy()\nTo explicitly construct a host matrix, consider using .to_numpy()."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_cambio_index = X.loc[X['default_1'] != Y].index\n",
        "X = X.drop('default_1', axis = 1)\n",
        "\n",
        "# Separo en entrenamiento y test\n",
        "X_train = X.loc[train_indices]\n",
        "Y_train = Y.loc[train_indices]\n",
        "X_test = X.loc[test_indices]\n",
        "Y_test = Y.loc[test_indices]\n",
        "\n",
        "X_test_cambio_index =list(set(X_test.index) & set(X_cambio_index))\n",
        "Y_test_cambio_index = list(set(Y_test.index) & set(X_cambio_index))\n",
        "\n",
        "X_test_cambio = X_test.loc[X_test_cambio_index]\n",
        "Y_test_cambio = Y_test.loc[Y_test_cambio_index]"
      ],
      "metadata": {
        "id": "473OsQ0bKDf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, X_test, Y_test, linear = None):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if linear:\n",
        "        y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
        "\n",
        "    cm = confusion_matrix(Y_test, y_pred)\n",
        "\n",
        "    precision = precision_score(Y_test, y_pred)\n",
        "    recall = recall_score(Y_test, y_pred)\n",
        "    f1 = f1_score(Y_test, y_pred)\n",
        "    accuracy = accuracy_score(Y_test, y_pred)\n",
        "    mse = mean_squared_error(Y_test, y_pred)\n",
        "\n",
        "    print(cm)\n",
        "    print(f'La precisión es: {precision}')\n",
        "    print(f'El recall es: {recall}')\n",
        "    print(f'El f1 es: {f1}')\n",
        "    print(f'El accuracy es: {accuracy}')\n",
        "    print(f'El MSE es: {mse} \\n')\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "ROeZ5eSKKFmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_table(model, elasticnet=None):\n",
        "    inverse_Cs = 1 / model.Cs_\n",
        "    results = []\n",
        "\n",
        "    if elasticnet:\n",
        "        l1_ratios = model.l1_ratios_\n",
        "        for c_idx, lambda_ in enumerate(inverse_Cs):\n",
        "            for l1_idx, l1_ratio in enumerate(l1_ratios):\n",
        "                    mean_score = np.mean(-model.scores_[1][:, c_idx, l1_idx], axis=0)\n",
        "                    std = np.std(-model.scores_[1][:, c_idx, l1_idx], axis=0)\n",
        "                    results.append({\n",
        "                        \"Lambda\": lambda_,\n",
        "                        \"L1 Ratio\": l1_ratio,\n",
        "                        \"Mean MSE\": mean_score,\n",
        "                        \"Std MSE\": std\n",
        "                    })\n",
        "\n",
        "        results_table = pd.DataFrame(results).sort_values(by=\"Mean MSE\", ascending=True)\n",
        "\n",
        "    else:\n",
        "        mean_scores = np.mean(-model.scores_[1], axis=0)\n",
        "        std = np.std(-model.scores_[1], axis=0)\n",
        "        results_table = pd.DataFrame({\n",
        "            \"Lambda\": inverse_Cs,\n",
        "            \"Mean Score\": mean_scores,\n",
        "            \"Std MSE\": std\n",
        "        }).sort_values(by=\"Mean Score\", ascending=True)\n",
        "\n",
        "    print(results_table)\n",
        "    return results_table"
      ],
      "metadata": {
        "id": "0PEr12GdKHn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_zero_coefs(model, X_train):\n",
        "    best_coefs = model.coef_[0]\n",
        "    feature_names = X_train.columns\n",
        "    non_zero_coefs = []\n",
        "    for coef, name in zip(best_coefs, feature_names):\n",
        "        if coef != 0:\n",
        "            non_zero_coefs.append({\n",
        "                \"Variable\": name,\n",
        "                \"Coeficiente\": coef\n",
        "            })\n",
        "    non_zero_table = pd.DataFrame(non_zero_coefs)\n",
        "    print(non_zero_table)\n",
        "\n",
        "    return non_zero_table"
      ],
      "metadata": {
        "id": "Ry2oKth9KLEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tscv = TimeSeriesSplit(n_splits=10)\n",
        "\n",
        "Cs = np.logspace(-4, 4, 20)"
      ],
      "metadata": {
        "id": "wUMA-64YKN9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_idx, test_idx in tscv.split(X_cudf):\n",
        "    X_train, X_test = X_cudf.iloc[train_idx], X_cudf.iloc[test_idx]\n",
        "    y_train, y_test = y_cudf.iloc[train_idx], y_cudf.iloc[test_idx]\n",
        "\n",
        "    # Mejor modelo por validación cruzada\n",
        "    best_C = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    for C in Cs:\n",
        "        model = LogisticRegression(penalty=\"l1\", solver=\"qn\", C=C, fit_intercept=True, max_iter=2000)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predicción y evaluación\n",
        "        y_pred = model.predict(X_test)\n",
        "        mse = ((y_test - y_pred) ** 2).mean()\n",
        "\n",
        "        if mse < best_score:\n",
        "            best_score = mse\n",
        "            best_C = C\n",
        "\n",
        "    errors.append(best_score)\n",
        "    print(f\"Fold terminado, mejor C: {best_C}, error: {best_score}\")\n",
        "\n",
        "print(\"Cross-validation finalizado. Errores por fold:\", errors)"
      ],
      "metadata": {
        "id": "x-AOJNStLvLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}